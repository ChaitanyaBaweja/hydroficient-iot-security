{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomaly Detection Lab — Grand Marina Water System\n",
    "\n",
    "This notebook trains and evaluates anomaly detection models on synthetic IoT sensor data\n",
    "matching the Grand Marina's water monitoring pipeline.\n",
    "\n",
    "**How to use this notebook:**\n",
    "1. Run all cells from top to bottom (`Runtime > Run all`)\n",
    "2. Observe the outputs — data generation, model training, and visualizations\n",
    "3. When you see `# === YOUR EXPERIMENT ===`, that's where you change values\n",
    "4. Record your results in the tables provided in the task instructions\n",
    "\n",
    "**You do not need to write code from scratch.** Everything is built for you.\n",
    "Your job is to run experiments, observe results, and draw conclusions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 1: Setup and Data Generation\n",
    "\n",
    "This section installs required libraries and generates synthetic sensor data\n",
    "that matches the ranges used by `publisher_defended.py` in Project 6."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Install required libraries (already available in Colab, but just in case)\n",
    "!pip install -q scikit-learn matplotlib numpy pandas joblib"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Libraries loaded successfully!\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# =============================================================================\n# Generate Synthetic Sensor Data\n# These ranges match publisher_defended.py from Project 6\n# =============================================================================\n\nN_NORMAL = 1000    # Normal sensor readings\nN_ANOMALY = 50     # Injected anomalies\n\n# --- Normal data ---\n# Pressure: 58-62 PSI (matches publisher_defended.py)\nnormal_pressure = np.random.uniform(58.0, 62.0, N_NORMAL)\n\n# Flow rate: 45-55 LPM, correlated with pressure (matches publisher_defended.py)\nnormal_flow = normal_pressure * 0.85 + np.random.normal(0, 1.5, N_NORMAL)\nnormal_flow = np.clip(normal_flow, 45.0, 55.0)\n\n# Gate position: 42-48% (matches publisher_defended.py)\nnormal_gate = np.random.uniform(42.0, 48.0, N_NORMAL)\n\n# --- Anomalous data (5 types) ---\nn_each = N_ANOMALY // 5\n\n# Type 1: Pressure spikes (>70 PSI — clearly outside 58-62 range)\nanom_pressure_1 = np.random.uniform(70.0, 85.0, n_each)\nanom_flow_1 = np.random.uniform(48.0, 55.0, n_each)  # normal flow\nanom_gate_1 = np.random.uniform(42.0, 48.0, n_each)\n\n# Type 2: Flow drops (<30 LPM — clearly outside 45-55 range)\nanom_pressure_2 = np.random.uniform(58.0, 62.0, n_each)  # normal pressure\nanom_flow_2 = np.random.uniform(15.0, 28.0, n_each)  # abnormally low\nanom_gate_2 = np.random.uniform(42.0, 48.0, n_each)\n\n# Type 3: Stuck sensors (identical readings — same value every time)\nanom_pressure_3 = np.full(n_each, 60.00)  # inside normal range but suspiciously constant\nanom_flow_3 = np.full(n_each, 51.00)\nanom_gate_3 = np.full(n_each, 45.00)\n\n# Type 4: Correlation breaks (high pressure + low flow — physically unlikely)\nanom_pressure_4 = np.random.uniform(68.0, 78.0, n_each)\nanom_flow_4 = np.random.uniform(15.0, 25.0, n_each)\nanom_gate_4 = np.random.uniform(42.0, 48.0, n_each)\n\n# Type 5: Slow drift (pressure climbing steadily from 62 up to 80)\nanom_pressure_5 = np.linspace(62.0, 80.0, n_each)\nanom_flow_5 = np.random.uniform(48.0, 55.0, n_each)\nanom_gate_5 = np.random.uniform(42.0, 48.0, n_each)\n\n# Combine all data\nall_pressure = np.concatenate([normal_pressure, anom_pressure_1, anom_pressure_2,\n                                anom_pressure_3, anom_pressure_4, anom_pressure_5])\nall_flow = np.concatenate([normal_flow, anom_flow_1, anom_flow_2,\n                           anom_flow_3, anom_flow_4, anom_flow_5])\nall_gate = np.concatenate([normal_gate, anom_gate_1, anom_gate_2,\n                           anom_gate_3, anom_gate_4, anom_gate_5])\n\n# Labels: 1 = normal, -1 = anomaly (matching sklearn convention)\nlabels = np.concatenate([np.ones(N_NORMAL), -np.ones(N_ANOMALY)])\n\n# Create feature matrix\nX = np.column_stack([all_pressure, all_flow, all_gate])\n\n# Shuffle the data\nshuffle_idx = np.random.permutation(len(X))\nX = X[shuffle_idx]\nlabels = labels[shuffle_idx]\n\n# Create a DataFrame for easier inspection\ndf = pd.DataFrame(X, columns=['pressure_psi', 'flow_rate_lpm', 'gate_position_pct'])\ndf['is_anomaly'] = (labels == -1)\n\nprint(f\"Dataset created:\")\nprint(f\"  Total points:   {len(X)}\")\nprint(f\"  Normal points:  {(labels == 1).sum()}\")\nprint(f\"  Anomaly points: {(labels == -1).sum()}\")\nprint(f\"\\nFeature ranges:\")\nprint(f\"  Pressure: {X[:, 0].min():.1f} - {X[:, 0].max():.1f} PSI\")\nprint(f\"  Flow:     {X[:, 1].min():.1f} - {X[:, 1].max():.1f} LPM\")\nprint(f\"  Gate:     {X[:, 2].min():.1f} - {X[:, 2].max():.1f} %\")\nprint(f\"\\nFirst 5 rows:\")\ndf.head()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 2: Baseline Isolation Forest Model\n",
    "\n",
    "Train a baseline Isolation Forest with default-ish parameters and see how it performs."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# Train Baseline Isolation Forest\n",
    "# =============================================================================\n",
    "\n",
    "# contamination = expected fraction of anomalies in the data\n",
    "# We know we injected ~5% anomalies (50 out of 1050)\n",
    "model = IsolationForest(contamination=0.05, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "start_time = time.time()\n",
    "model.fit(X)\n",
    "train_time = time.time() - start_time\n",
    "\n",
    "# Get predictions: 1 = normal, -1 = anomaly\n",
    "predictions = model.predict(X)\n",
    "\n",
    "# Get anomaly scores (lower = more anomalous)\n",
    "scores = model.decision_function(X)\n",
    "\n",
    "print(f\"Baseline Isolation Forest trained in {train_time:.3f} seconds\")\n",
    "print(f\"\\nPredictions:\")\n",
    "print(f\"  Classified as normal:  {(predictions == 1).sum()}\")\n",
    "print(f\"  Classified as anomaly: {(predictions == -1).sum()}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# Evaluate Baseline Results\n",
    "# =============================================================================\n",
    "\n",
    "def evaluate_model(true_labels, pred_labels, model_name=\"Model\"):\n",
    "    \"\"\"Calculate and print precision, recall, F1 for anomaly detection.\"\"\"\n",
    "    # Convert to binary: anomaly=1, normal=0 (for sklearn metrics)\n",
    "    true_binary = (true_labels == -1).astype(int)\n",
    "    pred_binary = (pred_labels == -1).astype(int)\n",
    "\n",
    "    tp = ((pred_binary == 1) & (true_binary == 1)).sum()\n",
    "    fp = ((pred_binary == 1) & (true_binary == 0)).sum()\n",
    "    fn = ((pred_binary == 0) & (true_binary == 1)).sum()\n",
    "    tn = ((pred_binary == 0) & (true_binary == 0)).sum()\n",
    "\n",
    "    precision = precision_score(true_binary, pred_binary, zero_division=0)\n",
    "    recall = recall_score(true_binary, pred_binary, zero_division=0)\n",
    "    f1 = f1_score(true_binary, pred_binary, zero_division=0)\n",
    "\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"  {model_name} Results\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"  True Positives  (correctly flagged):   {tp}\")\n",
    "    print(f\"  False Positives (normal flagged):      {fp}\")\n",
    "    print(f\"  False Negatives (anomalies missed):    {fn}\")\n",
    "    print(f\"  True Negatives  (correctly passed):    {tn}\")\n",
    "    print(f\"  ---\")\n",
    "    print(f\"  Precision: {precision:.3f}\")\n",
    "    print(f\"  Recall:    {recall:.3f}\")\n",
    "    print(f\"  F1 Score:  {f1:.3f}\")\n",
    "    print(f\"{'='*50}\")\n",
    "\n",
    "    return {\"precision\": precision, \"recall\": recall, \"f1\": f1,\n",
    "            \"tp\": tp, \"fp\": fp, \"fn\": fn, \"tn\": tn}\n",
    "\n",
    "baseline_results = evaluate_model(labels, predictions, \"Baseline Isolation Forest\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 3: Visualization\n",
    "\n",
    "Scatter plot showing normal data (blue) and detected anomalies (red)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# Scatter Plot: Pressure vs. Flow Rate\n",
    "# =============================================================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Plot 1: Ground truth (what we know)\n",
    "ax1 = axes[0]\n",
    "normal_mask = labels == 1\n",
    "anomaly_mask = labels == -1\n",
    "\n",
    "ax1.scatter(X[normal_mask, 0], X[normal_mask, 1], c='steelblue', s=10, alpha=0.5, label='Normal')\n",
    "ax1.scatter(X[anomaly_mask, 0], X[anomaly_mask, 1], c='red', s=30, alpha=0.8, label='Actual Anomaly', marker='x')\n",
    "ax1.set_xlabel('Pressure (PSI)')\n",
    "ax1.set_ylabel('Flow Rate (LPM)')\n",
    "ax1.set_title('Ground Truth: Actual Anomalies')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Model predictions (what the model sees)\n",
    "ax2 = axes[1]\n",
    "pred_normal = predictions == 1\n",
    "pred_anomaly = predictions == -1\n",
    "\n",
    "ax2.scatter(X[pred_normal, 0], X[pred_normal, 1], c='steelblue', s=10, alpha=0.5, label='Predicted Normal')\n",
    "ax2.scatter(X[pred_anomaly, 0], X[pred_anomaly, 1], c='red', s=30, alpha=0.8, label='Predicted Anomaly', marker='x')\n",
    "ax2.set_xlabel('Pressure (PSI)')\n",
    "ax2.set_ylabel('Flow Rate (LPM)')\n",
    "ax2.set_title('Model Predictions: Detected Anomalies')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Left plot: where we actually put anomalies\")\n",
    "print(\"Right plot: where the model thinks anomalies are\")\n",
    "print(\"Compare the two — how well does the model match reality?\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# Anomaly Score Distribution\n",
    "# =============================================================================\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "ax.hist(scores[normal_mask], bins=40, alpha=0.6, color='steelblue', label='Normal data')\n",
    "ax.hist(scores[anomaly_mask], bins=20, alpha=0.6, color='red', label='Actual anomalies')\n",
    "ax.axvline(x=0, color='black', linestyle='--', label='Decision boundary')\n",
    "ax.set_xlabel('Anomaly Score (lower = more anomalous)')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_title('Distribution of Anomaly Scores')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Points to the LEFT of the dashed line are classified as anomalies.\")\n",
    "print(\"Ideally, all red bars should be left of the line and all blue bars right of it.\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 4: Experiment — Contamination Sweep\n",
    "\n",
    "How does changing the `contamination` parameter affect detection performance?\n",
    "\n",
    "`contamination` tells the model what fraction of data is anomalous.\n",
    "Higher values = flag more data as anomalous."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# === YOUR EXPERIMENT ===\n",
    "# Try different contamination values and observe how results change.\n",
    "# The default is 0.05. Try values from very conservative (0.01) to aggressive (0.2).\n",
    "\n",
    "contamination_values = [0.01, 0.03, 0.05, 0.1, 0.2]\n",
    "\n",
    "# =============================================================================\n",
    "# Run the experiment (no changes needed below this line)\n",
    "# =============================================================================\n",
    "print(f\"{'Contamination':>14} | {'Detected':>8} | {'Precision':>9} | {'Recall':>6} | {'F1':>6}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "contamination_results = {}\n",
    "\n",
    "for c in contamination_values:\n",
    "    m = IsolationForest(contamination=c, random_state=42)\n",
    "    m.fit(X)\n",
    "    preds = m.predict(X)\n",
    "\n",
    "    true_bin = (labels == -1).astype(int)\n",
    "    pred_bin = (preds == -1).astype(int)\n",
    "\n",
    "    p = precision_score(true_bin, pred_bin, zero_division=0)\n",
    "    r = recall_score(true_bin, pred_bin, zero_division=0)\n",
    "    f = f1_score(true_bin, pred_bin, zero_division=0)\n",
    "    detected = (preds == -1).sum()\n",
    "\n",
    "    contamination_results[c] = {\"precision\": p, \"recall\": r, \"f1\": f, \"detected\": detected}\n",
    "\n",
    "    print(f\"{c:>14.2f} | {detected:>8} | {p:>9.3f} | {r:>6.3f} | {f:>6.3f}\")\n",
    "\n",
    "# Find best F1\n",
    "best_c = max(contamination_results, key=lambda k: contamination_results[k]['f1'])\n",
    "print(f\"\\nBest contamination by F1 score: {best_c} (F1 = {contamination_results[best_c]['f1']:.3f})\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 5: Experiment — Number of Trees (n_estimators)\n",
    "\n",
    "More trees = more stable results, but slower training.\n",
    "At what point do more trees stop helping?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# === YOUR EXPERIMENT ===\n",
    "# Try different numbers of trees.\n",
    "\n",
    "n_estimators_values = [50, 100, 200, 500]\n",
    "\n",
    "# =============================================================================\n",
    "# Run the experiment\n",
    "# =============================================================================\n",
    "print(f\"{'n_estimators':>12} | {'Precision':>9} | {'Recall':>6} | {'F1':>6} | {'Train Time':>10}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "estimator_results = {}\n",
    "\n",
    "for n in n_estimators_values:\n",
    "    m = IsolationForest(contamination=0.05, n_estimators=n, random_state=42)\n",
    "\n",
    "    start = time.time()\n",
    "    m.fit(X)\n",
    "    t = time.time() - start\n",
    "\n",
    "    preds = m.predict(X)\n",
    "    true_bin = (labels == -1).astype(int)\n",
    "    pred_bin = (preds == -1).astype(int)\n",
    "\n",
    "    p = precision_score(true_bin, pred_bin, zero_division=0)\n",
    "    r = recall_score(true_bin, pred_bin, zero_division=0)\n",
    "    f = f1_score(true_bin, pred_bin, zero_division=0)\n",
    "\n",
    "    estimator_results[n] = {\"precision\": p, \"recall\": r, \"f1\": f, \"time\": t}\n",
    "\n",
    "    print(f\"{n:>12} | {p:>9.3f} | {r:>6.3f} | {f:>6.3f} | {t:>9.3f}s\")\n",
    "\n",
    "best_n = max(estimator_results, key=lambda k: estimator_results[k]['f1'])\n",
    "print(f\"\\nBest n_estimators by F1 score: {best_n} (F1 = {estimator_results[best_n]['f1']:.3f})\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 6: Experiment — Sample Size (max_samples)\n",
    "\n",
    "How much data does each tree see? Smaller samples make trees more diverse."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# === YOUR EXPERIMENT ===\n",
    "# Try different sample sizes per tree.\n",
    "# 'auto' means min(256, n_samples)\n",
    "\n",
    "max_samples_values = ['auto', 64, 128, 256]\n",
    "\n",
    "# =============================================================================\n",
    "# Run the experiment\n",
    "# =============================================================================\n",
    "print(f\"{'max_samples':>12} | {'Precision':>9} | {'Recall':>6} | {'F1':>6}\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "samples_results = {}\n",
    "\n",
    "for s in max_samples_values:\n",
    "    m = IsolationForest(contamination=0.05, max_samples=s, random_state=42)\n",
    "    m.fit(X)\n",
    "    preds = m.predict(X)\n",
    "\n",
    "    true_bin = (labels == -1).astype(int)\n",
    "    pred_bin = (preds == -1).astype(int)\n",
    "\n",
    "    p = precision_score(true_bin, pred_bin, zero_division=0)\n",
    "    r = recall_score(true_bin, pred_bin, zero_division=0)\n",
    "    f = f1_score(true_bin, pred_bin, zero_division=0)\n",
    "\n",
    "    samples_results[str(s)] = {\"precision\": p, \"recall\": r, \"f1\": f}\n",
    "\n",
    "    print(f\"{str(s):>12} | {p:>9.3f} | {r:>6.3f} | {f:>6.3f}\")\n",
    "\n",
    "best_s = max(samples_results, key=lambda k: samples_results[k]['f1'])\n",
    "print(f\"\\nBest max_samples by F1 score: {best_s} (F1 = {samples_results[best_s]['f1']:.3f})\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 7: Best Configuration\n",
    "\n",
    "Combine the best values from your experiments and compare against the default."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# === YOUR EXPERIMENT ===\n",
    "# Fill in your best values from the experiments above.\n",
    "# Replace the ___ with the values that gave the best F1 scores.\n",
    "\n",
    "best_model = IsolationForest(\n",
    "    contamination=0.05,     # your best value from Section 4\n",
    "    n_estimators=100,       # your best value from Section 5\n",
    "    max_samples='auto',     # your best value from Section 6\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# =============================================================================\n",
    "# Compare default vs. best\n",
    "# =============================================================================\n",
    "\n",
    "# Default model\n",
    "default_model = IsolationForest(contamination=0.05, random_state=42)\n",
    "default_model.fit(X)\n",
    "default_preds = default_model.predict(X)\n",
    "\n",
    "# Your best model\n",
    "best_model.fit(X)\n",
    "best_preds = best_model.predict(X)\n",
    "\n",
    "print(\"DEFAULT MODEL:\")\n",
    "default_results = evaluate_model(labels, default_preds, \"Default Isolation Forest\")\n",
    "\n",
    "print(\"\\nYOUR BEST MODEL:\")\n",
    "best_results = evaluate_model(labels, best_preds, \"Your Best Configuration\")\n",
    "\n",
    "print(f\"\\n{'Metric':<12} | {'Default':>8} | {'Best':>8} | {'Change':>8}\")\n",
    "print(\"-\" * 45)\n",
    "for metric in ['precision', 'recall', 'f1']:\n",
    "    d = default_results[metric]\n",
    "    b = best_results[metric]\n",
    "    change = b - d\n",
    "    sign = '+' if change >= 0 else ''\n",
    "    print(f\"{metric:<12} | {d:>8.3f} | {b:>8.3f} | {sign}{change:>7.3f}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 8: LOF Comparison\n",
    "\n",
    "Compare Isolation Forest against Local Outlier Factor (LOF).\n",
    "\n",
    "LOF uses density estimation instead of isolation. Points in sparse\n",
    "neighborhoods (far from other points) are flagged as anomalies."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# LOF with default parameters\n",
    "# =============================================================================\n",
    "\n",
    "lof_default = LocalOutlierFactor(n_neighbors=20, contamination=0.05)\n",
    "\n",
    "start = time.time()\n",
    "lof_preds = lof_default.fit_predict(X)\n",
    "lof_time = time.time() - start\n",
    "\n",
    "print(\"LOF Default (n_neighbors=20):\")\n",
    "lof_default_results = evaluate_model(labels, lof_preds, \"LOF (n_neighbors=20)\")\n",
    "print(f\"  Training time: {lof_time:.3f}s\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# === YOUR EXPERIMENT ===\n",
    "# Try different n_neighbors values for LOF.\n",
    "\n",
    "n_neighbors_values = [10, 20, 50]\n",
    "\n",
    "# =============================================================================\n",
    "# Run the experiment\n",
    "# =============================================================================\n",
    "print(f\"{'n_neighbors':>12} | {'Precision':>9} | {'Recall':>6} | {'F1':>6} | {'Time':>8}\")\n",
    "print(\"-\" * 55)\n",
    "\n",
    "lof_results = {}\n",
    "\n",
    "for n in n_neighbors_values:\n",
    "    lof = LocalOutlierFactor(n_neighbors=n, contamination=0.05)\n",
    "\n",
    "    start = time.time()\n",
    "    preds = lof.fit_predict(X)\n",
    "    t = time.time() - start\n",
    "\n",
    "    true_bin = (labels == -1).astype(int)\n",
    "    pred_bin = (preds == -1).astype(int)\n",
    "\n",
    "    p = precision_score(true_bin, pred_bin, zero_division=0)\n",
    "    r = recall_score(true_bin, pred_bin, zero_division=0)\n",
    "    f = f1_score(true_bin, pred_bin, zero_division=0)\n",
    "\n",
    "    lof_results[n] = {\"precision\": p, \"recall\": r, \"f1\": f, \"time\": t}\n",
    "\n",
    "    print(f\"{n:>12} | {p:>9.3f} | {r:>6.3f} | {f:>6.3f} | {t:>7.3f}s\")\n",
    "\n",
    "best_lof_n = max(lof_results, key=lambda k: lof_results[k]['f1'])\n",
    "print(f\"\\nBest n_neighbors by F1: {best_lof_n} (F1 = {lof_results[best_lof_n]['f1']:.3f})\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# Head-to-Head Comparison: IF vs. LOF\n",
    "# =============================================================================\n",
    "\n",
    "# Use best IF results from Section 7 and best LOF from above\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"  HEAD-TO-HEAD: Isolation Forest vs. Local Outlier Factor\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"\\n{'Metric':<30} | {'IF (Best)':>10} | {'LOF (Best)':>10}\")\n",
    "print(\"-\" * 58)\n",
    "print(f\"{'Precision':<30} | {best_results['precision']:>10.3f} | {lof_results[best_lof_n]['precision']:>10.3f}\")\n",
    "print(f\"{'Recall':<30} | {best_results['recall']:>10.3f} | {lof_results[best_lof_n]['recall']:>10.3f}\")\n",
    "print(f\"{'F1 Score':<30} | {best_results['f1']:>10.3f} | {lof_results[best_lof_n]['f1']:>10.3f}\")\n",
    "print(f\"{'Training Time':<30} | {train_time:>9.3f}s | {lof_results[best_lof_n]['time']:>9.3f}s\")\n",
    "print(f\"{'Can score new data one at a time?':<30} | {'Yes':>10} | {'No':>10}\")\n",
    "print(f\"{'Needs full dataset present?':<30} | {'No':>10} | {'Yes':>10}\")\n",
    "\n",
    "# Declare winner\n",
    "if best_results['f1'] >= lof_results[best_lof_n]['f1']:\n",
    "    print(f\"\\nOverall winner by F1: Isolation Forest\")\n",
    "else:\n",
    "    print(f\"\\nOverall winner by F1: Local Outlier Factor\")\n",
    "\n",
    "print(f\"\\nNote: Even if LOF has slightly better F1, Isolation Forest is\")\n",
    "print(f\"the better choice for real-time IoT because it can score new\")\n",
    "print(f\"messages one at a time as they arrive from the MQTT broker.\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 9: Optional — One-Class SVM\n",
    "\n",
    "**This section is optional.** Skip it if you're short on time.\n",
    "\n",
    "One-Class SVM draws a boundary around normal data in feature space.\n",
    "Points outside the boundary are anomalies."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# Optional: One-Class SVM Comparison\n",
    "# =============================================================================\n",
    "\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# SVM needs scaled features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Train One-Class SVM (nu parameter is similar to contamination)\n",
    "svm_model = OneClassSVM(nu=0.05, kernel='rbf', gamma='scale')\n",
    "\n",
    "start = time.time()\n",
    "svm_model.fit(X_scaled)\n",
    "svm_time = time.time() - start\n",
    "\n",
    "svm_preds = svm_model.predict(X_scaled)\n",
    "\n",
    "print(\"One-Class SVM:\")\n",
    "svm_results = evaluate_model(labels, svm_preds, \"One-Class SVM\")\n",
    "print(f\"  Training time: {svm_time:.3f}s\")\n",
    "\n",
    "# Updated comparison table\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"  THREE-WAY COMPARISON\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"\\n{'Metric':<25} | {'IF':>8} | {'LOF':>8} | {'SVM':>8}\")\n",
    "print(\"-\" * 58)\n",
    "print(f\"{'Precision':<25} | {best_results['precision']:>8.3f} | {lof_results[best_lof_n]['precision']:>8.3f} | {svm_results['precision']:>8.3f}\")\n",
    "print(f\"{'Recall':<25} | {best_results['recall']:>8.3f} | {lof_results[best_lof_n]['recall']:>8.3f} | {svm_results['recall']:>8.3f}\")\n",
    "print(f\"{'F1 Score':<25} | {best_results['f1']:>8.3f} | {lof_results[best_lof_n]['f1']:>8.3f} | {svm_results['f1']:>8.3f}\")\n",
    "print(f\"{'Training Time':<25} | {train_time:>7.3f}s | {lof_results[best_lof_n]['time']:>7.3f}s | {svm_time:>7.3f}s\")\n",
    "print(f\"{'Real-time scoring?':<25} | {'Yes':>8} | {'No':>8} | {'Yes':>8}\")\n",
    "print(f\"{'Needs scaling?':<25} | {'No':>8} | {'No':>8} | {'Yes':>8}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 10: Export Model\n",
    "\n",
    "Save your trained Isolation Forest model to a file.\n",
    "You'll download this and use it in `subscriber_dashboard_ai.py`."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# =============================================================================\n# Export the Best Isolation Forest Model\n# =============================================================================\n\nimport os\nimport joblib\n\n# Use the best model from Section 7\n# (If you changed the parameters there, this exports YOUR best model)\nexport_model = best_model\n\n# Save to file\nmodel_path = 'anomaly_model.joblib'\njoblib.dump(export_model, model_path)\n\nprint(f\"Model saved to: {model_path}\")\nprint(f\"File size: {os.path.getsize(model_path) / 1024:.1f} KB\")\nprint(f\"\\nTo use this model:\")\nprint(f\"  1. Download {model_path} from Colab (Files panel > right-click > Download)\")\nprint(f\"  2. Place it in your Project 8 directory\")\nprint(f\"  3. Run subscriber_dashboard_ai.py — it will load the model automatically\")\n\n# Quick verification: load it back and score one point\nloaded_model = joblib.load(model_path)\ntest_point = np.array([[60.0, 50.0, 45.0]])  # normal reading\ntest_pred = loaded_model.predict(test_point)\ntest_score = loaded_model.decision_function(test_point)\nprint(f\"\\nVerification — test point [60 PSI, 50 LPM, 45%]:\")\nprint(f\"  Prediction: {'Normal' if test_pred[0] == 1 else 'ANOMALY'}\")\nprint(f\"  Score: {test_score[0]:.3f}\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# =============================================================================\n# Download Helper (for Google Colab)\n# =============================================================================\n\ntry:\n    from google.colab import files\n    files.download('anomaly_model.joblib')\n    print(\"Download started! Check your browser's download folder.\")\nexcept ImportError:\n    print(\"Not running in Colab — find anomaly_model.joblib in your working directory.\")\nexcept Exception as e:\n    print(f\"Auto-download failed: {e}\")\n    print(\"Manually download: Files panel (left sidebar) > anomaly_model.joblib > Download\")",
   "execution_count": null,
   "outputs": []
  }
 ]
}